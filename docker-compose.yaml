services:
  cache-db1:
    image: redis/redis-stack:latest
    ports:
      - "6379:6379"
    volumes:
      - cache_db1_data:/data

  cache-db2:
    image: redis/redis-stack:latest
    ports:
      - "16379:6379"
    volumes:
      - cache_db2_data:/data

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama

  model-loader:
    image: curlimages/curl:latest
    command: "curl http://ollama:11434/api/pull -d '{ \"name\": \"bge-m3\" }'"
    depends_on:
      - ollama

  embeddings-api:
    image: "langcache-embeddings-api:0.0.3"
    ports:
      - "8081:11434"

  langcache-provisioner:
    image: "langcache:0.0.12"
    entrypoint: /provision-cache-index
    command: -config=/config.yaml -ignore
    depends_on:
      - cache-db1
    volumes:
      - ./config.yaml:/config.yaml:ro

  langcache-redis:
    image: "langcache:0.0.12"
    entrypoint: /server
    command: --config=/config.yaml
    depends_on:
      - cache-db1
      - langcache-provisioner
      - embeddings-api
      - ollama
      - model-loader
    ports:
      - "8080:8080"
    volumes:
      - ./config.yaml:/config.yaml:ro

  llm-app:
    build: ./llm-app
    container_name: llm-app
    image: llm-app:latest
    ports:
      - "5001:8080"
    depends_on:
      - langcache-redis
      - cache-db1
      - ollama
      - embeddings-api
    env_file:
      - .env

volumes:
  ollama_data:
  cache_db1_data:
  cache_db2_data:
